%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the all SemEval submissions
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

%Title format for system description papers by task participants
\title{Duluth at SemEval-2017 Task 6:  Language Models in Humor Detection}
%Title format for task description papers by task organizers
%\title{SemEval-2017 Task [TaskNumber]:  [Task Name]}


\author{Xinru Yan \\
  Department of Computer Science \\ University of Minnesota Duluth \\ Duluth, MN, 55812 USA \\
  {\tt yanxx418@d.umn.edu} \\\And
  Ted Pedersen \\
  Department of Computer Science \\ University of Minnesota Duluth \\ Duluth, MN, 55812 USA \\
  {\tt tpederse@d.umn.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper describes the Duluth system that participated in SemEval-2017 Task 6 \#HashtagWars: Learning a Sense of Humor. The system completed Subtask A and Subtask B using N-gram language models, ranking well during evaluation. This paper includes the results of our system during development and evaluation stage, with several post-evaluation results. 
\end{abstract}

\section{Introduction}
Since humor represents human uniqueness and intelligence to some extent, it has continuously drawn attention in different research areas such as linguistics, psychology, philosophy and computer science. In computer science, relevant theories derived from those fields have formed a relatively young area of study, \textit{computational humor} \cite{Recognizing:Humor:On:Twitter}. Humor has not been addressed broadly in current computational research. Many studies have developed decent systems to produce humor \cite{ozbal2012computational}. However, \textit{humor detection} is essentially a more challenging and fun problem. For example, Mihalcea and Strapparava draw particular attention on automatic humor recognition in their work \cite{Learning:To:Laugh}. SemEval-2017 Task 6 focuses on \textit{humor detection} by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, \textit{@midnight with Chris Hardwick}. Our system, Duluth, applies the language model approach to detect humor by training N-gram language models on two sets of training data, the tweets data and the news data.


\section{Background}
Training \textbf{Language models} (LMs) is a straightforward way to collect set of rules by utilizing the fact that words do not appear in an arbitrary order, which means we can gain useful information from a word and its neighbors ~\cite{JM}. A statistical language model is a model that computes the probability of a sequence of words or an upcoming word ~\cite{JM}. Below are two examples of language modeling:

To compute the probability of a sequence of words $W$ given the sequence $(w_{1},w_{2},w_{3})$, we have:
\begin{equation}
P(W) = P(w_{1},w_{2},w_{3})
\end{equation}

To compute the probability of an upcoming word $w_3$ given the sequence $(w_{1},w_{2})$, we have:
\begin{equation}
P(w_{3}|w_{1},w_{2})
\end{equation}

The idea of word prediction with probabilistic models is called the N-gram model, which predicts the upcoming word from the previous N-1 words. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence of words and a trigram is a three-word sequence of words. For example, in tweet ``tears in Ramen \#SingleLifeIn3Words'', ``tears'', ``in'', ``Ramen'' and ``\#SingleLifeIn3Words'' are unigrams; ``tears in'', ``in Ramen'' and ``Ramen \#SingleLifeIn3Words'' are bigrams and ``tears in Ramen'' and ``in Ramen \#SingleLifeIn3Words'' are trigrams.

When we use for example, a trigram LM, to predict the conditional probability of the next word, we are thus making the following approximation:
\begin{equation}
P(w_n|w_1^{n-1})\approx P(w_n|w_{n-2}, w_{n-1})
\end{equation}

The assumption that the probability of a word depends only on a small number of previous words is called the \textbf{Markov} assumption ~\cite{markov1954theory}. According to the Markov assumption, here we show the general equation for computing the probability of a complete word sequence using a trigram LM:
\begin{equation}
P(w_1^n)\approx \prod_{k=1}^{n} P(w_k|w_{k-2}, w_{k-1})
\end{equation}

In the study on how phrasing affects memorability, in order to analyze the characteristics of memorable quotes, researchers take the language model approach to investigate the distinctiveness feature ~\cite{hello}. Specifically, to evaluate how distinctive a quote is, they evaluate its likelihood with the respect of the ``common language'' model which consists of the newswire sections of the Brown corpus. They employ LMs on the ``common language'' and come to the conclusion that movie quotes which are less like the ``common language'' are more memorable. The idea of using LMs to assess the memorability of a quote is suitable for our purpose of detecting how humorous a tweet is. Except for using tweets provided by the task to train N-gram LMs, our system also trained N-gram LMs on English news data in order to evaluate how distinctive, in this case, how funny, a tweet is comparing to the tweets model and the news model. Tweets that are more like the tweets model, or less like the news model, are ranked as being more funny. For our purpose, we trained bigram LMs and trigram LMs on both sets of training data.

KenLM, as a language modeling tool, is used in our system ~\cite{Heafield-estimate}. LMs are estimated from the corpus using modified Kneser-Ney smoothing without pruning. KenLM reads a text file and generates LMs in ARPA format. KenLM also implements back-off technique, which means if the N-gram is not found, it applies the lower order N-gram's probability along with its back-off weights. Instead of using the real probability of the N-gram, KenLM applies the logarithm scheme.

\section{Method}
Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:
\begin{enumerate}
\item Corpus preparation and pre-processing: Collected all training data files to form one training corpus. Pre-processing included filtering and tokenization.
\item Language model training: Built N-gram LMs by feeding the corpus to the KenLM Language Model Toolkit. 
\item Tweet scoring: Computed log probability for each tweet based on the trained N-gram LM.
\item Tweet prediction: Based on the log probability
\begin{itemize}
\item Subtask A -- Given two tweets, compared them and predicted which one is funnier. 
\item Subtask B -- Given a set of tweets associated with one hashtag, ranked tweets from the funnest to the least funny.
\end{itemize}
\end{enumerate}

\subsection{Corpus Preparation and Pre-processing}
The Duluth system uses two distinct sets of training data: the tweets data and the news data. The tweets data was provided by the SemEval task. It consisted of 106 hashtag files with about 21,000 tokens. We collected in total of 6.2 GB of English news data with about 2,000,000 tokens from the News Commentary Corpus and the News Crawl Corpus from 2008, 2010 and 2011 \footnote{http://www.statmt.org/wmt11/translation-task.html\#download}.   
\subsubsection{Preparation}
To prepare the tweets training corpus, the system took each tweet from the hashtag files, which included tweets from both \textit{train\_dir} and \textit{trial\_dir} from the task, and created a text file with each tweet on its own line. During the development stage of the system we trained LMs solely on the \textit{train\_dir} data, which included 100 hashtag files; we tested the system on the \textit{trial\_dir} data consisting of 6 hashtag files. For the news data, the system read each sentence from the news files and created a text file with each sentence per line to form the news training corpus. 
\subsubsection{Pre-processing}
The pre-processing consisted of two steps: filtering and tokenization. The filtering step was only for the tweet training corpus. We experimented with various filtering and tokenziation combinations during the development stage to determine the best setting. 
\begin{itemize}
\item Filtering: the filtering process included removing the following elements from the tweets:
\begin{itemize}
\item URLs
\item Twitter user names: Tokens starting with the ``@'' symbol 
\item Hashtags: Tokens starting with the ``\#'' symbol 
\end{itemize}
\item Tokenization: For both training data sets we split text by spaces and punctuation marks
\end{itemize}

\subsection{Language Model Training}
Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram LMs on each corpus. We trained two different LMs, bigrams and trigrams, on both tweets and news training data sets. Table 1 shows an example ARPA file of the trigram LM we trained on the tweets data:

\begin{table}[h!]
\begin{tabular}{ |l |c|}
\hline
N-gram 1 = 21580 \\
N-gram 2 = 60624 \\
N-gram 3 = 73837 \\
\hline
unigram:\\
-4.8225346   \textless unk\textgreater{}   0 \\
0   \textless s\textgreater{}   -0.47505832 \\
-1.4503417   \textless /s\textgreater{}   0 \\
-4.415446   Donner  -0.12937292 \\
...\\
\hline
bigrams:\\
...\\
-0.9799023  Drilling Gulf -0.024524588\\
...\\
\hline
trigrams:\\
...\\
-1.171928 I'll start thinking\\
...\\
\hline
\end{tabular}
\caption{An example ARPA file of a trigram LM trained on the tweets data}
\label{table:1}
\end{table}

The ARPA file starts with a header, listing the number of each N-gram. Each N-gram line starts with the logarithm probability of that N-gram, followed by the N-gram which consists of N words. The logarithm of the back-off weight for the N-gram optionally follows after. Notice that there are three ``special'' words in a language model: the beginning of a sentence denoted by \textless s\textgreater, the end of a sentence denoted by \textless /s\textgreater{} and the out of vocabulary word denoted by \textless unk\textgreater. In order to be able to handle the unknown words to estimate the probability of a tweet more accurately, in all our experiments we kept the \textless unk\textgreater{} word in our LMs. To derive the best setting of the LMs for both tasks, we experimented using the language model with and without sentence boundaries.

\subsection{Tweet Scoring}
After training the N-gram LMs, the next step was scoring. For each hashtag file that needed to be evaluated, based on the trained N-gram LM, a logarithm score was assigned by our system for each tweet in the hashtag file. The larger the score, the more likely that the tweet appeared with respect to that LM. Table 2 shows an example of two scored tweets from hashtag file \textit{Bad\_Job\_In\_5\_Words.tsv} based on the trigram LM trained on the tweets data.

\begin{table*}[h!]
\centering
\begin{tabular}{ |p{4.7cm}|p{4.7cm}|p{4.7cm}| } 
\hline
\multicolumn{3}{|c|}{The hashtag: \#BadJobIn5Words} \\
\hline
tweet\_id & tweet & score \\
\hline 
705511149970726912 & The host of Singled Out \#BadJobIn5Words @midnight & -19.923433303833008 \\
\hline
705538894415003648 & Donut receipt maker and sorter  \#BadJobIn5Words @midnight & -27.67446517944336 \\
\hline
\end{tabular}
\caption{Scored tweet according to the trigram LM. The format follows .tsv file provided by the task. The first column shows tweets\_id; the second column shows tweets; the third column shows the probability score computed based on the trigram LM. }
\label{table:2}
\end{table*}

\begin{table*}[h!]
\centering
\begin{tabular}{ |p{1.2cm}|p{1.2cm}|p{1.2cm}|p{1.7cm}|p{1.5cm}|p{1.9cm}|p{1.7cm}|p{1.7cm}|}
\hline
DataSet & N-gram & \# \& @ removed  & Sentence Boundaries & Lowercase & Tokenization & Subtask A Accuracy & Subtask B Distance \\
\hline
tweets & trigram & False & False & False & False & 0.543 & 0.887 \\
\hline
tweets & trigram & False & True & True & False & 0.522 & 0.900 \\
\hline
tweets & bigram & False & False & False & False & 0.548 & 0.900 \\ 
\hline
news & trigram & NA & False & False & True & 0.539 & 0.923 \\
\hline
news & trigram & NA & False & False & False & 0.460 & 0.923 \\
\hline
news & bigram & NA & False & False & False & 0.470 & 0.900 \\
\hline
\end{tabular}
\caption{Development results. The development results are based on data from \textit{trial\_dir}. In general, trigram LMs outperform bigram LMs.}
\label{table:3}
\end{table*}

\begin{table*}[h!]
\centering
\begin{tabular}{ |p{1.2cm}|p{1.2cm}|p{1.2cm}|p{1.7cm}|p{1.5cm}|p{1.9cm}|p{1.7cm}|p{1.7cm}|}
\hline
DataSet & N-gram & \# \& @ removed  & Sentence Boundaries & Lowercase & Tokenization & Subtask A Accuracy & Subtask B Distance \\
\hline
tweets & trigram & False & False & False & False & 0.397 & 0.967 \\
\hline
tweets & bigram & False & False & False & False & 0.406 & 0.944 \\
\hline
news & trigram & NA & False & False & True & 0.627 & 0.872 \\
\hline
news & bigram & NA & False & False & True & 0.624 & 0.853 \\
\hline
\end{tabular}
\caption{Evaluation and post-evaluation results. The evaluation and post-evaluation results are based on evaluation data from \textit{gold\_dir}. The trigram LM trained on the news data ranked 4th place for Subtask A and 1st place for Subtask B during evaluation.}
\label{table:4}
\end{table*}

\subsection{Tweet Prediction}
The system sorted tweets for each hashtag file based on their score, meaning the funniest one was listed on the top i.e. if the system used a tweets LM, the tweets would be sorted in descending order. In the case that it used a news LM, the tweets would be sorted in ascending order. For Subtask A, given a hashtag file, the system went through the sorted list of tweets, compared each pair of tweets and produced a tsv format file. For each tweet pair, if the first tweet was funnier than the second one, the system would output the tweet\_ids for the pair followed by ``1''. Otherwise it output the tweet\_ids followed by ``0''. For Subtask B, given a hashtag file, the system output the tweet\_ids starting from the funniest.


\section{Experiments and Results}
In this section we present the results from the development stage (\textit{Table 3}), the evaluation stage (\textit{Table 4}), as well as two post-evaluation results (\textit{Table 4}). Since we implemented both bigram and trigam LMs during the development stage but only results from trigram LMs were submitted to the task, we evaluated bigram LMs in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 3 and Table 4 are provided by the task. 

Table 3 shows results from the development stage. From this table we can estimate the best setting to train LMs for both data sets: for the tweets data we decided to use trigrams and omit sentence boundaries; for the news data we chose to train trigram LMs on a tokenized news corpus.

Table 4 shows the results of our system applying trigram LMs during evaluation along with bigram LMs results from the post-evaluation runs. It demonstrates that trigram LMs work better than bigram LMs.


\section{Discussion and Future Work}
We focused on training bigram and trigram LMs because tweets are normally short and concise. Trigrams outperform bigrams because trigrams have relatively better coverage than bigrams. 

After comparing the amount of tweets data and news data we used, we believe that the lack of tweets data could have caused the tweets LMs to perform worse. Therefore, one way to improve the system, especially the tweets data LM, is to collect more tweets that participate in the hashtag wars. We would also like to train news LMs using the same amount of data we have for the tweets to see how the results compare. Additionally, we want to gather more news data and see if the quantity of news data would still make a difference. 

Besides, we would like to try some machine learning techniques, specifically deep learning methods such as recurrent neural networks. Studies have shown that neural network based LMs work effectively and outperform standard back-off N-gram models ~\cite{mikolov2011extensions}. In addition, recurrent neural networks are capable of forming short term memory so it can better deal with problems associated with sequences. It would be interesting to see if some combination of these methods could enhance the system.




% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{semeval2017}
\bibliographystyle{acl_natbib}


\end{document}
