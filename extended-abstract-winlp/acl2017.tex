\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Who's to say what's funny? A computer using Language Models and Deep Learning, that's who!}

\author{Xinru Yan \& Ted Pedersen\\
  Department of Computer Science \\ University of Minnesota Duluth \\ Duluth, MN, 55812 USA \\
  {\tt \{yanxx418,tpederse\}@d.umn.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Humor is a defining characteristic of human beings. 
Our goal is to develop methods that automatically 
detect humorous statements and rank them on a 
continuous scale. In this paper we report on results 
using a Language Model approach, and outline our
plans for using methods from Deep Learning.
\end{abstract}

\section{Introduction}
\textit{Computational humor} is an emerging area of 
research that ties together ideas from psychology, 
linguistics, and cognitive science. 
\textit{Humor generation} is the problem of automatically
creating humorous statements 
(e.g., \cite{StockS03}, \cite{ozbal2012computational}).
\textit{Humor detection} seeks to identify humor in text, 
and is sometimes cast as a binary classification problem that 
decides if some input is humorous or not 
(e.g., \cite{Learning:To:Laugh}, 
\cite{Recognizing:Humor:On:Twitter}, \cite{ShahafHM15},
\cite{MillerG15}). However, our focus is on  
the continuous and subjective aspects of humor. 

We learn a particular sense of humor from a data set of
tweets which are geared towards a certain style of humor
\cite{2016hashtagwars}. 
This data consists of humorous tweets which have been
submitted in response to hashtag prompts provided
during the Comedy
Central TV show \textit{@midnight with Chris Hardwick}.
Since not all jokes are equally funny, we use Language
Models and methods from Deep Learning to allow 
potentially humorous statements to be ranked relative to 
each other. 

\section{Language Models}

We used traditional Ngram language models as our first 
approach for two reasons : 
\begin{enumerate}
\item Ngram language models can learn a certain style of 
humor by using examples of that as the training data for the model, and
\item they assign a probability to each input they are given,
making it possible to rank statements relative to each other.  
\end{enumerate}
Thus, Ngram language models make relative rankings 
of humorous statements
based on a particular style of humor, thereby accounting for the 
continuous and subjective nature of humor. 

We began this research by participating in SemEval-2017 Task 6 
\#HashtagWars: Learning a Sense of Humor \cite{PotashRR17}. This included
two subtasks : Pairwise Comparison (Subtask A) and Semi-ranking 
(Subtask B). Pairwise comparison asks a system to choose the funnier of
two tweets. Semi-ranking requires that each of the tweets
associated with a particular hashtag be assigned to one of the 
following categories : top most funny tweet, next nine most funny
tweets, and all remaining tweets. 

%% submission should be anonymous, so anything that gives us away should
% be kept out til the final version

%%The Duluth system
%%\footnote{https://xinru1414.github.io/HumorDetection-SemEval2017-Task6/} 
Our system
estimated tweet probabilities using Ngram language models.
%% \cite{XT}. 
We created models
from two different corpora - a collection of funny tweets from the @midnight
program, and a corpus of news data that is freely available for 
research\footnote{http://www.statmt.org/wmt11/featured-translation-task.html}. 
We scored tweets by assigning them a probability based on each 
model. Tweets that have a higher probability according to the funny 
tweet model are considered funnier since they are more like the humorous
training data. However, tweets 
that have a lower probability according to the news language model 
are viewed as funnier since they are least like the (unfunny) news corpus.
We took a standard approach to language modeling and used bigrams 
and trigrams as features in our models. We used KenLM \cite{Heafield-estimate} 
with modified Kneser-Ney smoothing and a back-off technique as our language 
modeling tool. 

Table 1 shows our results for both data sets when trained on 
bigrams and trigrams. The accuracy and distance measures
are defined by the task organizers \cite{PotashRR17}. We seek 
high accuracy in picking the funnier tweet (Subtask A) and low
distance (from the gold standard) in organizing the tweets into 
categories (Subtask B). 

\begin{table}[h]
\begin{center}
\begin{tabular}{cccc}
%%\begin{tabular}{ |p{1.2cm}|p{1.2cm}|p{1.7cm}|p{1.7cm}|}
\hline
Data & Ngram & Accuracy (A) & Distance (B) \\
\hline
tweets & trigram & 0.397 & 0.967 \\
tweets & bigram & 0.406 & 0.944 \\
news & trigram & 0.627 & 0.872 \\
news & bigram & 0.624 & 0.853 \\
\end{tabular}
\caption{Experimental results}
\end{center}
\end{table}

These results show that models trained on the news data
have a significant advantage over the tweets model, and that bigram
models performed slightly better than trigrams. We submitted
trigram models trained on news and tweets to the official evaluation 
of SemEval-2017 Task 6. The trigram language models trained on
the news data placed fourth in Subtask A and first in Subtask B.

We believe that the significant advantage of the news data over 
the tweet data is caused by the much larger quantity of news data
available. The tweet data only consists of approximately 21,000 tweets,
whereas the news data totals approximately 6.2 GB of text.
In the future we intend to collect more tweet data, especially those 
participating in the ongoing \#HashtagWars staged nightly by @midnight. 
We also plan to experiment with equal amounts of tweet data and
news data, to see if one has an inherent advantage over the other.

Finally, our language models performed better on subtask A, the pairwise
comparison, suggesting that the ranking problem is more demanding
and will require further development. To that end we are considering the
use of neural network and Deep Learning approaches, which we will 
discuss in the following section.

\section{Deep Learning}

Our system performed well in the SemEval task, confirming the ability 
of language models to detect humor with focusing on its continuous 
and subjective nature. Going forward, language models certainly 
can characterize humor in a more comprehensive way with more 
complex implementations.

Recently, studies have shown that Recurrent Neural Networks 
(RNN) such as LSTM neural networks are exceptionally powerful 
in language modeling based on its ability to take into account 
of all preceding words over a word sequence \cite{LSTM1} 
\cite{LSTM2}. Also, in their work of learning a sense of humor, 
\cite{2016hashtagwars} points out that due to the high portion 
of out of vocabulary (OOV) words generated by puns in the 
humorous tweet data, character-level Constitutional Neural 
Networks (CNN) model would be more suitable for capturing the 
single-token puns comparing to token level models. In addition, 
\cite{CNN}'s study proves that character-level neural language 
models outperform state-of-art word-level neural language 
models. Moving forward, considering the promising future of 
neural networks, we would like to build a character-level neural 
language model (NLM) which relies on both LSTM RNN and character-level 
CNN to improve our current system performance.

Last but not the least, \cite{2016hashtagwars} states that 
external knowledge is crucial in order to detecting humor from 
the tweet dataset based on its nature. In the future we intend 
to make use of external knowledge in the NLM in possibly two 
ways: With a deeper understanding of what kinds of external 
knowledge are the most useful such as movie and book titles, 
celebrity and song names, one is to incorporate them as features 
in the NLM; The other is to combine the NLM with a Neural 
Turing Machine (NTM) \cite{graves2014neural}, which is particularly 
designed for interacting with external interfaces, with a 
profound and solid study of NTM.

\section{Conclusion}

Humor has not been addressed broadly in current computational 
research area. Our research focuses on humor detection, developing 
systems that can capture its continuous and subjective nature. 
 
% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\end{document}
